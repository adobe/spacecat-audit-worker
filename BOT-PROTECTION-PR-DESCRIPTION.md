# Bot Protection Detection and Audit Abortion - Audit Worker

## ğŸ¯ Summary

This PR implements bot protection detection in the Audit Worker by querying CloudWatch logs from the Content Scraper. When bot protection is detected, audits are automatically aborted to avoid wasting resources on blocked sites.

**Key Feature**: Bot protection checking is implemented in the `StepAudit` base class, which means **all audits** (meta-tags, cwv, broken-backlinks, accessibility, etc.) automatically get this functionality without any audit-specific code changes.

---

## ğŸš€ What's Changed

### Core Implementation

1. **Universal Bot Protection Check** (`src/common/step-audit.js`)
   - Added `checkBotProtection()` async function (lines 28-115)
   - Queries CloudWatch logs for `[BOT-BLOCKED]` events
   - Aggregates statistics by HTTP status code and blocker type
   - Returns early to abort audit when bot protection is detected
   - Integrated into `run()` method (lines 231-244) to check before processing scrape results

2. **CloudWatch Query Utility** (`src/utils/cloudwatch-utils.js`)
   - New utility file for CloudWatch Logs interactions
   - `queryBotProtectionLogs(siteId, context, searchStartTime)` function
   - Filters by `siteId` and `[BOT-BLOCKED]` prefix
   - Applies 5-minute buffer to handle clock skew and log ingestion delays
   - Queries up to 500 events (increased from 100)
   - Parses JSON log messages and extracts bot protection metadata

3. **Comprehensive Testing**
   - `test/common/step-audit.test.js`: Bot protection scenarios, edge cases
   - `test/utils/cloudwatch-utils.test.js`: CloudWatch query tests
   - âœ… 100% code coverage maintained

---

## ğŸ” How It Works

### Audit Flow with Bot Protection

```
1. Audit Step 1 completes â†’ Audit created (auditedAt = T0)
2. Audit Step 2 creates scrape job
3. Content Scraper runs and logs bot protection to CloudWatch
4. Audit Step 3 starts:
   a. Load scrape result paths
   b. âœ… Check CloudWatch for bot protection logs (T0 - 5min to now)
   c. If detected â†’ âŒ Abort audit with detailed statistics
   d. If not detected â†’ âœ… Continue audit normally
```

### CloudWatch Query Strategy

- **Filter**: `"[BOT-BLOCKED]" "${siteId}"`
- **Time Window**: `audit.getAuditedAt() - 5 minutes` to `now`
- **Why 5 minutes?**: Handles clock skew and CloudWatch log ingestion delays
- **Why audit timestamp?**: Set before scraping starts, captures all relevant logs

### Log Message Format

When bot protection is detected:
```
[BOT-BLOCKED] Audit aborted for type meta-tags for site https://example.com (site-123) 
with bot protection details: HTTP Status Counts: [403: 5, 200: 2], 
Blocker Types: [cloudflare: 5, akamai: 2], 
Bot Protected URLs: [https://example.com/page1, https://example.com/page2, ...]
```

---

## âœ… Benefits

### 1. Universal Coverage
- âœ… All audits that use scraping automatically get bot protection detection
- âœ… No changes needed in individual audit implementations
- âœ… Future audits inherit this functionality automatically

### 2. Resource Efficiency
- âœ… Audits abort immediately when bot protection is detected
- âœ… No wasted processing on blocked sites
- âœ… Clear feedback about why audit cannot proceed

### 3. Detailed Diagnostics
- âœ… HTTP status code distribution (403, 200, etc.)
- âœ… Blocker type distribution (Cloudflare, Akamai, etc.)
- âœ… List of affected URLs
- âœ… Easy debugging and investigation

### 4. Clean Architecture
- âœ… No coupling between services via SQS message flags
- âœ… No DynamoDB schema changes needed
- âœ… CloudWatch logs as single source of truth
- âœ… No `scrape.json` files created for bot-protected URLs

---

## ğŸ“‹ Affected Audits

The following audits automatically benefit from bot protection detection:

- âœ… `meta-tags`
- âœ… `cwv` (Core Web Vitals)
- âœ… `broken-backlinks`
- âœ… `accessibility`
- âœ… `prerender`
- âœ… `readability`
- âœ… `structured-data`
- âœ… `page-citability`
- âœ… All future audits that use scraping

**No audit-specific code changes required!**

---

## ğŸ§ª Testing

### Test Coverage
- âœ… 100% line coverage
- âœ… 100% branch coverage
- âœ… 100% statement coverage

### Test Scenarios
- âœ… Bot protection detected â†’ audit aborts
- âœ… No bot protection â†’ audit continues
- âœ… Missing httpStatus field â†’ defaults to 'unknown'
- âœ… Missing blockerType field â†’ defaults to 'unknown'
- âœ… Malformed log messages â†’ gracefully skipped
- âœ… CloudWatch query errors â†’ proper error handling
- âœ… Empty results â†’ audit continues

---

## ğŸ“¦ Dependencies

### Added
- `@aws-sdk/client-cloudwatch-logs`: AWS SDK for CloudWatch Logs queries

### Environment Variables
- `CONTENT_SCRAPER_LOG_GROUP`: CloudWatch log group name (default: `/aws/lambda/spacecat-services--content-scraper`)
- `AWS_REGION`: AWS region for CloudWatch client

---

## ğŸ”— Related Changes

This PR is part of a coordinated effort across multiple services:

1. **Content Scraper**: Detects bot protection, logs to CloudWatch, does not create `scrape.json`
2. **Audit Worker** (this PR): Reads CloudWatch logs, aborts audits
3. **Task Processor**: Reads CloudWatch logs, sends Slack alerts, aborts processing
4. **API Service**: Uses shared library for bot detection during onboarding

---

## ğŸ“– Documentation

- **Implementation Details**: See `BOT-PROTECTION-IMPLEMENTATION.md`
- **Flow Diagrams**: Included in documentation
- **Configuration Guide**: Environment variables and constants

---

## ğŸ¬ Example Scenarios

### Scenario 1: Bot Protection Detected

**Input**: Meta-tags audit for `example.com`, Content Scraper detected Cloudflare blocking 5/10 URLs

**Output**:
```json
{
  "status": 200,
  "body": {
    "skipped": true,
    "reason": "bot-protection-detected",
    "botProtectedUrlsCount": 5,
    "totalUrlsCount": 10,
    "stats": {
      "totalCount": 5,
      "byHttpStatus": { "403": 5 },
      "byBlockerType": { "cloudflare": 5 }
    }
  }
}
```

**Log**:
```
[BOT-BLOCKED] Audit aborted for type meta-tags for site https://example.com (site-123) 
with bot protection details: HTTP Status Counts: [403: 5], 
Blocker Types: [cloudflare: 5], 
Bot Protected URLs: [https://example.com/page1, ...]
```

### Scenario 2: No Bot Protection

**Input**: CWV audit for `safe-site.com`, no bot protection logs found

**Output**: Audit completes normally, opportunities created

**Log**: `No bot protection logs found for site safe-site.com`

---

## ğŸš¦ Deployment Notes

### Prerequisites
- âœ… Content Scraper must be deployed with bot protection logging
- âœ… CloudWatch log group must be accessible
- âœ… AWS credentials must have `logs:FilterLogEvents` permission

### Rollout
- âœ… Backward compatible (no breaking changes)
- âœ… Works independently if Content Scraper hasn't been updated (no logs = no detection)
- âœ… No infrastructure changes required

### Monitoring
- Check CloudWatch logs for `[BOT-BLOCKED]` entries
- Monitor audit completion rates (should increase as bot-protected audits abort faster)
- Verify Slack alerts are sent by Task Processor

---

## ğŸ” Code Review Focus Areas

1. **Time Window Logic**: Verify `audit.getAuditedAt()` is always before scraping starts
2. **CloudWatch Query**: Ensure filter pattern matches Content Scraper log format
3. **Error Handling**: Verify CloudWatch errors are caught and logged
4. **Test Coverage**: Confirm all branches and edge cases are tested
5. **Performance**: Verify CloudWatch queries are efficient (5-minute window, indexed by time)

---

## ğŸ“ Checklist

- [x] Bot protection detection implemented in `StepAudit` base class
- [x] CloudWatch query utility created and tested
- [x] All tests passing with 100% coverage
- [x] Linter errors fixed
- [x] Documentation created (`BOT-PROTECTION-IMPLEMENTATION.md`)
- [x] No breaking changes
- [x] Backward compatible with existing audits
- [x] Works for all audit types automatically

---

## ğŸ¯ Success Metrics

After deployment, we expect:
- âœ… Reduced wasted audit processing on bot-protected sites
- âœ… Faster feedback to users about bot protection issues
- âœ… Detailed bot protection statistics for debugging
- âœ… Clearer logs for investigating customer issues
- âœ… Consistent bot protection handling across all audits

---

## ğŸ“š Additional Resources

- [Bot Protection Flow Diagram](./docs/bot-protection-flow.md)
- [CloudWatch Logs Documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/)
- [SpaceCat Shared Utils - detectBotBlocker](https://github.com/adobe/spacecat-shared/tree/main/packages/spacecat-shared-utils#detectbotblocker)
